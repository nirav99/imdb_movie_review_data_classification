{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc55d95-ba04-4893-8a75-c54bb7de013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdabb3-e490-45ff-8176-ca1a68a0d073",
   "metadata": {},
   "source": [
    "Load the IMDB dataset.\n",
    "Create a tokenizer from the pre-trained BERT uncased model.\n",
    "Use the pre-trained BERT uncased model with 2 labels, 1 for positive class and 0 for negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0975f7fe-c4cd-4b67-bab6-9d9aeeba34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0ec2fc-c93e-4a71-a03b-78d3f829b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_specific_dataset(data, data_type):\n",
    "    \"\"\"\n",
    "    Examine some data. Count the total number of positive and negative samples and print some random samples.\n",
    "    \"\"\"\n",
    "    total_records = len(data)\n",
    "    num_positive_labels = np.sum(data['label'])\n",
    "    num_negative_labels = total_records - num_positive_labels\n",
    "    print(f\"Dataset Type : {data_type}, Total Records : {total_records}, Positive : {num_positive_labels}, Negative : {num_negative_labels}\")\n",
    "\n",
    "    samples = random.sample(range(0, total_records), 7)\n",
    "\n",
    "    for i in samples:\n",
    "        text = data['text'][i]\n",
    "        label = data['label'][i]\n",
    "        print(f\"Movie Review : {text}\\n  Label : {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5b2b2-06d2-41a3-9cfc-29e4aa40d350",
   "metadata": {},
   "source": [
    "Describe the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9435bb2b-8e2e-4766-8441-fc44ec021ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataset(dataset):\n",
    "    describe_specific_dataset(dataset['train'], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff1df76-2583-4699-9a2b-ea2ee4321730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Type : train, Total Records : 25000, Positive : 12500, Negative : 12500\n",
      "Movie Review : Flavia(Florinda Bolkan of \"Don't Torture a Duckling\" fame)is locked away in a convent of carnal desires by her father.Tired of all of the sadism she sees around her(rape of a young woman in a pigsty,sexual cravings,horse castration)Flavia decides to run from the convent with her Jewish friend from the outside,Abraham.The two don't get very far before they are captured and then brought back to be tortured and forced to repent.After punishment she joins up with a band of Muslims called the Tarantulas,who had invaded the convent prior and leads a crusade that turns into nothing short of a bloody battle behind the convent walls.\"Flavia the Heretic\" is a well-directed and fairly notorious piece of Italian nunsploitation.The film is slightly gruesome and sleazy at times.The acting is great and the characters are well-developed.Overall,\"Flavia the Heretic\" is a genuinely moving and intelligent movie with plenty of nudity and gore.You can't go wrong with it.8 out of 10.\n",
      "  Label : 1\n",
      "\n",
      "Movie Review : I have seen many good Korean Movies including thrillers and movies with darker overtone, but this one sucks. The director seems to be a sadist, who happened to get someone to produce some junk. The movie lacks any sort of entertainment value and is not even a thriller. I can't believe someone really made such a movie. Even though acting is OK, the story line and the feeling it leaves is awful.<br /><br />I am sure, I am not going to see any movies of this director. No sense of movie making, and utter disappointment in having thriller moments. All this has is showing scenes with psychopath wasting the reels with badly shot scenes and showing more blood and violence thinking that makes it thrilling. Very disappointing movie and I strongly recommend skipping all the movies of this sort.\n",
      "  Label : 0\n",
      "\n",
      "Movie Review : This wasn't really a very good movie. There were lots of implausible and predictable things that happened during the course of the film...but I think that most of the reviewers are missing the point of why this movie should be enjoyed by a wide audience. THIS MOVIE WAS PRODUCED BY MAGIC JOHNSON! Isn't that enough to inspire us all to check out this film? A film produced by a former NBA star doesn't come along every day, you know. Beautifully stupid kids in a big house getting slashed by an axe wielding psycho. Every cliché trotted out for us to groan over. Teen sex. And it was all produced by MAGIC JOHNSON! I can't say enough about this movie! Teen drug use! College hijinx! And it was all produced by MAGIC JOHNSON! Yippeeee!\n",
      "  Label : 0\n",
      "\n",
      "Movie Review : By the end of the first hour my jaw was nestled comfortably between my feet. The movie never, and I do mean never, lets up in action. It may be mild action but it's action. Once again every member of the cast fits perfectly. The explosions were realistic, the chase scenes were feasible, and the fighting was incredible. Matt Damon will forever be Jason Bourne.<br /><br />All I really have to say is that every Bourne movie gets better and this is no exception. The action, the stakes, the plot. How they do it I will never know. I applaud the man who wrote the screen plays to every one of these movies. Because if he hadn't done such a great job with the first movie, we wouldn't have this one to talk about.<br /><br />So don't go see it in theaters, go experience it in theaters if it's still out where you live, but if not December 11th Bourne comes home to you!\n",
      "  Label : 1\n",
      "\n",
      "Movie Review : Two years later... Bill (Alex Winter) and Ted (Keanu Reeves) are becoming near rock stars in the present future but still needing more work in their instruments. In the future, Bill & Ted are in the public popular history but then a evil man (Joss Ackland) is set to kill Bill & Ted by sending cyborg look-likes to destroy them. Cyborgs are sent to the past present and they actually murder the real Bill & Ted. Now, Both guys are spirits and they have to travel through Heaven and Hell to save themselves and their future.<br /><br />Directed by Peter Hewitt (Tom and Huck, The Borrowers) made a clever sequel with terrific visual effects. Much more funny and entertaining than the original. William Sadler (The Shawshank Redemption) steals the show as The Grim Reaper.<br /><br />DVD has an good anamorphic Widescreen (1.85:1) transfer and an fine-Dolby Digital 5.1 Surround Sound. DVD has the theatrical trailer and an amusing behind the scenes featurette. This sequel was a Box Office hit like the original but it is also (Believe it or not), one of the best sequels ever made (depending on your point of view). George Carlin reprises his role from the original briefly. Pam Grier also appears in a bit role. It's a enjoyable fantasy comedy. (****/*****).\n",
      "  Label : 1\n",
      "\n",
      "Movie Review : I bought this film on DVD so I could get an episode of Mystery Science Theater 3000. Thankfully, Mike, Crow, and Tom Servo are watchable, because the film itself is not. Although there is a plot, a story one can follow, and a few actors that can act, there isn't anything else. The movie was so boring, I have firmly confirmed that I will never watch it again without Tom, Crow and Mike. As summarized above, however, it was better than the film featured in the MST3K episode that preceded it; Mitchell.\n",
      "  Label : 0\n",
      "\n",
      "Movie Review : First of all, this movie reminded me of the old movies I used to have to watch in religion class in school. That's NOT a good thing. Basically, it's just a preachy and pretentious piece of filth, just like the terrible \"Left Behind\" series. I'm not offended by religious movies... but I am offended when these religious movies just happen to be extremely awful. I would just like to be able to say nice things about a christian movie but it doesn't look like that will happen any time soon. I bet if you gave the bible thumpers a decent budget, they still wouldn't be able to come up with anything good. Just avoid this one. Also, the fact that the \"American Family Association\" (basically, Reverend Wildmon's lackies) beam about this film on their website is another reason to make me hate it. In fact, after I viewed this, I went home and watched my copy of David Cronenberg's NC-17 rated \"Crash\". Forgive me father for I have sinned. Hahahahaha!\n",
      "  Label : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098736b-c3b6-4677-bf81-42ef6e4adec4",
   "metadata": {},
   "source": [
    "Define a tokenizer function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d9d150-1f6c-4ed5-8708-b4fc93a969ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd29e3d1a040c6b42081656d351a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "print(tokenized_dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf3c918-2ab9-46eb-8909-6abd9fdd2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch expects the class lablels in a column named \"labels\".\n",
    "set_format torch converts the required columns to pytorch tensors\n",
    "\"\"\"\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b62d3d3-5792-4157-ba71-22b0d6184f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\", # Run evaluation after every epoch\n",
    "    save_strategy=\"epoch\", # Save model checkpoint after every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01, # L2 regularization\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100, # Logs training metrics after 100 steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd2f80b-2ab6-40a3-ace0-2bae6094546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)\n",
    "    f1_score = f1.compute(predictions=preds, references=labels, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"f1\": f1_score[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e042e3-676f-4180-9487-f3d58a326b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle(seed=42).select(range(15000)),  # Subset for speed\n",
    "    eval_dataset=tokenized_dataset[\"test\"].select(range(3000)),  # Subset for speed\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3debbf-8956-46e4-9e37-4dceb8175b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='1876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1876/1876 1:10:37, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.360051</td>\n",
       "      <td>0.857667</td>\n",
       "      <td>0.923381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.274243</td>\n",
       "      <td>0.909333</td>\n",
       "      <td>0.952514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1876, training_loss=0.2751789418364893, metrics={'train_runtime': 4240.4903, 'train_samples_per_second': 7.075, 'train_steps_per_second': 0.442, 'total_flos': 7893331660800000.0, 'train_loss': 0.2751789418364893, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8002ba94-47c3-4719-a534-ee0ddd01231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.274243026971817,\n",
       " 'eval_accuracy': 0.9093333333333333,\n",
       " 'eval_f1': 0.952513966480447,\n",
       " 'eval_runtime': 109.6613,\n",
       " 'eval_samples_per_second': 27.357,\n",
       " 'eval_steps_per_second': 3.42,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a28986-41d6-4e87-89ac-41125e8081bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a boring and a poorly directed film.\n",
      "  Logits : tensor([[ 2.4590, -2.4597]]), Argmax : 0\n",
      "  Probabilities : tensor([[0.9927, 0.0073]]), confidence : 0.9927443265914917\n",
      "Sentiment : Negative\n",
      "\n",
      "I really enjoyed this movie. The story, the plot and the acting was superlative. The ending felt a little stretched, \n",
      "    could have been reduced, but I liked it overall\n",
      "  Logits : tensor([[-2.7041,  2.5022]]), Argmax : 1\n",
      "  Probabilities : tensor([[0.0055, 0.9945]]), confidence : 0.9945477843284607\n",
      "Sentiment : Positive\n",
      "\n",
      "OMG ! What a collosal waste of time ! Avoid !!!\n",
      "  Logits : tensor([[ 2.2393, -2.3877]]), Argmax : 0\n",
      "  Probabilities : tensor([[0.9903, 0.0097]]), confidence : 0.9903112649917603\n",
      "Sentiment : Negative\n",
      "\n",
      "Das war ein wirklich lustiger Film, die Charaktere haben gut gespielt. Es hat Spaß gemacht, ihn anzuschauen.\n",
      "  Logits : tensor([[-2.3759,  2.2326]]), Argmax : 1\n",
      "  Probabilities : tensor([[0.0099, 0.9901]]), confidence : 0.9901312589645386\n",
      "Sentiment : Positive\n",
      "\n",
      "यह मूवी बहुत ख़राब है।  उन्होंने कुछ भी बना दिया है।  दो घंटे और तीस मिनट बर्बाद कर दिए।  मत देखो। \n",
      "  Logits : tensor([[ 0.2697, -0.4739]]), Argmax : 0\n",
      "  Probabilities : tensor([[0.6778, 0.3222]]), confidence : 0.6777908802032471\n",
      "Sentiment : Negative\n",
      "\n",
      "Este foi um dos melhores filmes que assisti nos últimos meses. A história é ótima e a reviravolta no final me prendeu na ponta da cadeira. A cinematografia, a música ambiente e as atuações também são incríveis.\n",
      "  Logits : tensor([[-2.8125,  2.5670]]), Argmax : 1\n",
      "  Probabilities : tensor([[0.0046, 0.9954]]), confidence : 0.9954113364219666\n",
      "Sentiment : Positive\n",
      "\n",
      "C'est l'un des meilleurs films que j'ai vus depuis des mois. L'histoire est géniale et le rebondissement final m'a tenu en haleine. La photographie, la musique de fond et le jeu des acteurs sont également exceptionnels.\n",
      "  Logits : tensor([[-2.9148,  2.5639]]), Argmax : 1\n",
      "  Probabilities : tensor([[0.0042, 0.9958]]), confidence : 0.9958425164222717\n",
      "Sentiment : Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best saved model.\n",
    "model_path = \"./results/checkpoint-1876\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "def predict_sentiment(review):\n",
    "    \"\"\"\n",
    "    Given a review, compute the class probability (confidence) and the class.\n",
    "    1 means positive review and 0 means negative review.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    print(f\"  Logits : {logits}, Argmax : {torch.argmax(logits)}\")\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    confidence = probs[0][predicted_class].item()\n",
    "    print(f\"  Probabilities : {probs}, confidence : {confidence}\")\n",
    "    return label_map[predicted_class]\n",
    "\n",
    "movie_reviews = [\n",
    "    \"This was a boring and a poorly directed film.\",\n",
    "    \"\"\"I really enjoyed this movie. The story, the plot and the acting was superlative. The ending felt a little stretched, \n",
    "    could have been reduced, but I liked it overall\"\"\",\n",
    "    \"\"\"OMG ! What a collosal waste of time ! Avoid !!!\"\"\",\n",
    "    \"\"\"Das war ein wirklich lustiger Film, die Charaktere haben gut gespielt. Es hat Spaß gemacht, ihn anzuschauen.\"\"\",\n",
    "    \"\"\"यह मूवी बहुत ख़राब है।  उन्होंने कुछ भी बना दिया है।  दो घंटे और तीस मिनट बर्बाद कर दिए।  मत देखो। \"\"\",\n",
    "    \"\"\"Este foi um dos melhores filmes que assisti nos últimos meses. A história é ótima e a reviravolta no final me prendeu na ponta da cadeira. A cinematografia, a música ambiente e as atuações também são incríveis.\"\"\",\n",
    "    \"\"\"C'est l'un des meilleurs films que j'ai vus depuis des mois. L'histoire est géniale et le rebondissement final m'a tenu en haleine. La photographie, la musique de fond et le jeu des acteurs sont également exceptionnels.\"\"\"\n",
    "]\n",
    "\n",
    "for movie_review in movie_reviews:\n",
    "    print(movie_review)\n",
    "    print(f\"Sentiment : {predict_sentiment(movie_review)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d737de-5b65-405f-b0e3-a53e39117346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
